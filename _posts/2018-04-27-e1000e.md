---
layout: post
title:  "Linux Network Driver Case Study: Intel e1000e Ethernet Controller"
date:   2018-04-27 11:37:01 -0600
categories: linux kernel
---
I got questions from some of our customers on how to improve their device bandwidth. Althouth there are tons of reasons a design performs suboptimal, one quick check is the [NAPI](https://wiki.linuxfoundation.org/networking/napi). NIC driver used to process packet by packet. Evertime a packet arrives, controller get interrupted and processes it. At Gigabit speed senario, there are too much interrupt burden. The idea behind NAPI is to buffer the packets, then the software polls the buffer to pickup packets in a batch. Today's hardwares mostly are NAPI compatible, it is vitally important to take advantage of NAPI by careful design of the controller software. To illustrate how to use NAPI when designing controller software, I take the Intel e1000e Gigabit Ethernet Controller as an example. 

Intel e1000e driver is for 82574 family Gigabit Ethernet. Here is the [source code](https://elixir.bootlin.com/linux/v4.6/source/drivers/net/ethernet/intel/e1000e/netdev.c)
<script src="https://gist.github.com/siruix/64a698f3c4bebcbc1f5672f6501be6e0.js"></script>

1. DMA

2. irq && napi_schedule
```c
e1000_intr(ivoid *data){
	u32 icr = er32(ICR); //read ICR register
	//....
	struct net_device *netdev = data;
	struct e1000_adapter *adapter = netdev_priv(netdev); //read HW private data
	__napi_schedule(&adapter->napi); 
}
```
3. napi_poll & napi_complete
```c
e1000e_poll(struct napi_struct *napi, int weight){
	struct e1000_adapter *adapter = container_of(napi, struct e1000_adapter, napi);
	e1000_clean_tx_irq(adapter->tx_ring);
	adapter->clean_rx(adapter->rx_ring, &work_done, weight);
	if(work_done < weight){
		napi_complete(napi);
		e1000_irq_enable(adapter);
	}
	return work_done;
}
e1000_clean_rx_irq(struct e1000_ring *rx_ring){
	int count = 0;
	int i = tx_ring->next_to_clean;
	int eop = tx_ring->buffer_info][i].next_to_watch;
	struct e1000_buffer *buffer_info;
	struct e1000_tx_desc *tx_desc, *eop_desc;
	eop_desc = E1000_TX_DESC(*tx_ring, eop);
	while(eop_desc->upper.data & cpu_to_le32(E1000_TXD_STAT_DD) && (count < tx_ring->count)){
		for(; i != eop; count++){
			tx_desc = E1000_TX_DESC(*tx_ring, i);
			buffer_info = &tx_ring->buffer_info[i];
			e1000_put_txbuf(tx_ring, buffer_info);
			tx_desc->upper.data = 0;
			i++;
			if(i == tx_ring->count)
				i = 0;
		}
		if(i == tx_ring->next_to_use)
			break;
		eop = tx_ring->buffer_info[i].next_to_watch;
		eop_desc = E1000_TX_DESC(*tx_ring, eop);
	}
	tx_ring->net_to_clean = i;
}
```
4. napi_gro_receive

